{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 1. torch 이용 만든 텐서\n",
    "data = [[1,2], [3,4]]\n",
    "print(type(data))\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "print(x_data)\n",
    "\n",
    "# 2. numpy - > torch tensor\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones tensor:\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "random tensor: \n",
      " tensor([[0.3587, 0.5539],\n",
      "        [0.5109, 0.2222]])\n"
     ]
    }
   ],
   "source": [
    "#다른 텐서로부터 생성하기\n",
    "#명시적으로 재정의(override)하지 않으면, 인자로 주어진 텐서\n",
    "x_ones = torch.ones_like(x_data) #x_data 속성 유지\n",
    "print(f'ones tensor:\\n{x_ones}')\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) #x_data의 속성을 덮어씀\n",
    "print(f'random tensor: \\n {x_rand}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random tensor: tensor([[0.9664, 0.7736, 0.7654],\n",
      "        [0.4671, 0.1694, 0.0869]])\n",
      "\n",
      "ones tensor: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "zeros tensor: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#무작위 또는 상수 값을 나타내기 (randon & constant)\n",
    "#shape은 텐서의 dimension 을 나타내는 튜플로, 아래 함수들에서는 출력 텐서의 차원을 결정함\n",
    "shape = (2,3,)\n",
    "rand_tensor =  torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f'random tensor: {rand_tensor}\\n')\n",
    "print(f'ones tensor: {ones_tensor}\\n')\n",
    "print(f'zeros tensor: {zeros_tensor}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1665, 0.7635, 0.1578, 0.3028],\n",
      "        [0.6854, 0.9071, 0.8438, 0.1183],\n",
      "        [0.3146, 0.7622, 0.2320, 0.0063]])\n",
      "cpu\n",
      "shape of tensor: torch.Size([3, 4])\n",
      "datatype of tensor: torch.float32\n",
      "device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "#텐서의 속성(attribute)\n",
    "#텐서의 속성은 텐서의 모양(shape), 자료형(datatype) 및 어느 장치에 저장되는지를 나타냄\n",
    "tensor = torch.rand(3,4)\n",
    "print(tensor)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "tensor.to(device)\n",
    "\n",
    "print(f'shape of tensor: {tensor.shape}')\n",
    "print(f'datatype of tensor: {tensor.dtype}')\n",
    "print(f'device tensor is stored on: {tensor.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device tensor is sotred on: cpu\n"
     ]
    }
   ],
   "source": [
    "#텐서 연산\n",
    "'''\n",
    "전치(transposing), 인덱싱(indexing), 슬라이싱(slicing), 수학 계산, 선형 대수, 임의 샘플링(random sampling) 등\n",
    " 100가지 이상의 텐서 연산들을 여기에서 확인할 수 있음\n",
    " 각 연산들은 (일반적으로 cpu보다 빠른) gpu에서 실행 할 수 있음\n",
    " colab을 사용한다면, edit -> notebook settings 에서 gpu 할당이 가능\n",
    " '''\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.io('cuda')\n",
    "print(f'device tensor is sotred on: {tensor.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0.]])\n",
      "tensor([[1., 2., 1., 1.],\n",
      "        [1., 2., 1., 1.],\n",
      "        [1., 2., 1., 1.],\n",
      "        [1., 2., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 표준 인덱싱과 슬라이싱\n",
    "tensor_1 = torch.ones(4,4)\n",
    "tensor_1[:,3] = 0\n",
    "\n",
    "tensor_2 = torch.ones(4,4)\n",
    "tensor_2[:,1] = 2\n",
    "print(tensor_1)\n",
    "print(tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#텐서 합치기\n",
    "#torch.cat 을 사용해 주어진 차원에 따라 일련의 텐서를 연결 가능\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.]])\n",
      "tensor([[1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.],\n",
      "        [1., 2., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#곱하기\n",
    "t_mult = tensor_1.mul(tensor_2)\n",
    "print(t_mult)\n",
    "\n",
    "print(tensor_1*tensor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.]])\n",
      "tensor([[7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.],\n",
      "        [7., 7., 7., 7.]])\n"
     ]
    }
   ],
   "source": [
    "#행렬 곱\n",
    "print(tensor_2.matmul(tensor_2.T))\n",
    "print(tensor_2@tensor_2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "tensor * tensor \n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#텐서 곱하기\n",
    "#요소별 곱(element-wise product)을 계산함\n",
    "print(f'tensor.mul(tensor) \\n{tensor.mul(tensor)}\\n')\n",
    "\n",
    "#다른 문법\n",
    "print(f'tensor * tensor \\n{tensor * tensor}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 4., 4., 4.],\n",
      "        [0., 4., 4., 4.],\n",
      "        [0., 4., 4., 4.]])\n",
      "\n",
      "tensor @ tensor.T \n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 4., 4., 4.],\n",
      "        [0., 4., 4., 4.],\n",
      "        [0., 4., 4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "#두 텐서 간의 행렬 곱(matrix multiplication)을 계산\n",
    "print(f'tensor.matmul(tensor.T) \\n{tensor.matmul(tensor.T)}\\n')\n",
    "# 다른 문법:\n",
    "print(f'tensor @ tensor.T \\n{tensor @ tensor.T}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(t)\n",
    "n = t.numpy()\n",
    "print(n)\n",
    "\n",
    "t.add(1)\n",
    "print(t)\n",
    "print(n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뷰(view) - 원소의 수를 유지하며 텐서의 크기 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.],\n",
      "         [ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "파이토치 텐서의 뷰는 넘파이의 reshape와 같은 역할\n",
    "reshape ->텐서의 크기를 변경해주는 역할\n",
    "'''\n",
    "\n",
    "t_temp = np.array([[[0,1,2],[3,4,5],[6,7,8], [9,10,11]]])\n",
    "ft = torch.FloatTensor(t_temp)\n",
    "print(ft)\n",
    "print(ft.shape)\n",
    "\n",
    "#이제 ft view -> 2차원 텐서로 변경\n",
    "# -1 : 나는 그 값을 모르겠음. 파이토치 니가 알아서 해\n",
    "#(두번째 차원은 길이는 3 가지도록 하라는 의미임.)\n",
    "#결론 : 현재 3차원 텐서를 2차원 텐서로 변경하되 (?, 3)의 크기로 변경하라는 의미임.\n",
    "# 결과적으로 (4,3)의 크기를 가지는 텐서를 얻었음.\n",
    "# 내부적으로 크기 변환은 다음과 같이 이루어짐. (2, 2, 3) -> (2x 2, 3) -> (4,3)\n",
    "print(ft.view([-1, 3])) # (?,3)\n",
    "print(ft.view([-1, 3]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "스퀴즈 -> 1차원을 제거\n",
    "스퀴즈는 차원이 1인 경우에는 해당 차원을 제거함.\n",
    "실습 3x1 크기를 가지는 2차원 텐서 생성\n",
    "'''\n",
    "\n",
    "ft = torch.FloatTensor(([0], [1], [2]))\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "해당 텐서는 (3x1)의 크기를 가짐\n",
    "두번째 차원이 1이므로 squeeze를 사용하면 (3, )의 크기를 가지는 텐서로 변경됨.\n",
    "'''\n",
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)\n",
    "\n",
    "#위의 결과는 1이었는 두번째 차원이 제거되며, \n",
    "#(3, )의 크기를 가지는 텐서로 변경되어 1차원 벡터가 된 것을 보여줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([[0, 1, 2]])\n",
      "torch.Size([1, 3])\n",
      "tensor([[0, 1, 2]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "#언스퀴즈 - 특정 위치에 1인 차원을 추가함\n",
    "# ppt 285 설명 참조\n",
    "\n",
    "ft=torch.tensor([0,1,2])\n",
    "print(ft.shape)\n",
    "\n",
    "print(ft.unsqueeze(0))\n",
    "print(ft.unsqueeze(0).shape)\n",
    "\n",
    "print(ft.view(1, -1))\n",
    "print(ft.view(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(1))\n",
    "print(ft.unsqueeze(1).shape)\n",
    "\n",
    "print(ft.unsqueeze(-1))\n",
    "print(ft.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ft_temp.view(1,-1))\n",
    "print(ft_temp.view(-1,1)).shape\n",
    "\n",
    "tensor([[0.,1.,2.]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
